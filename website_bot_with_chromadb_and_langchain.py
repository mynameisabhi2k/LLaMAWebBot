# -*- coding: utf-8 -*-
"""Website_bot_with_ChromaDB_and_LangChain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iBYTh7uuuMihvqy0m2S1vMrg-QBIHWY8

#**1: Install All the Required Packages**
"""

!pip -q install langchain
!pip -q install bitsandbytes accelerate transformers
!pip -q install datasets loralib sentencepiece
!pip -q install pypdf
!pip -q install sentence_transformers
!pip -q install -U langchain-community

!pip -q install unstructured

!pip install tokenizers

!pip install xformers

!pip install chromadb

"""#**2: Import All the Required Libraries**"""

from langchain.document_loaders import UnstructuredURLLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import Chroma
import chromadb
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import pipeline
from langchain import HuggingFacePipeline
from huggingface_hub import notebook_login
import textwrap
import sys
import os
import torch

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

"""#**3: Pass the URLs and extract the data from these URLs**"""

URLs=[
    'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb',
    'https://www.mosaicml.com/blog/mpt-7b',
    'https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models',
    'https://lmsys.org/blog/2023-03-30-vicuna/'

]

loaders = UnstructuredURLLoader(urls=URLs)
data = loaders.load()

data[0]

len(data)

"""#**4: Split the Text into Chunks**"""

text_splitter=CharacterTextSplitter(separator='\n',
                                    chunk_size=1000,
                                    chunk_overlap=200)

text_chunks=text_splitter.split_documents(data)

len(text_chunks)

text_chunks[0]

text_chunks[1]

text_chunks[2]

"""#**5: Download the Hugging Face Embeddings**"""

#embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
embeddings=HuggingFaceEmbeddings()

embeddings

query_result = embeddings.embed_query("How are you")
len(query_result)

# query_result

"""#**06: Convert the Text Chunks into Embeddings and Create a Knowledge Base**"""

vectorstore = Chroma.from_documents(documents=text_chunks, embedding=embeddings, persist_directory="./chroma_db")

vectorstore.persist()



index_name='llama'

"""#**07: Create a Large Language Model (LLM) Wrapper - Llama**"""

notebook_login()

#model1 = "meta-llama/Llama-2-7b-chat-hf"
model_name = "meta-llama/Llama-3.2-1B"

tokenizer = AutoTokenizer.from_pretrained(model_name,
                                          use_auth_token=True)

pipe = pipeline("text-generation",
                model=model_name,
                tokenizer= tokenizer,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                max_new_tokens = 512,
                do_sample=True,
                top_k=30,
                num_return_sequences=1,
                eos_token_id=tokenizer.eos_token_id
                )

llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})

llm.predict("Please provide a concise summary of the anime One Piece")

"""#**08: Initialize the Retrieval QA with Source Chain**"""

from langchain.chains import RetrievalQA

query = "How good is Vicuna?"

docs = vectorstore.similarity_search(query, k=3)

docs

qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=vectorstore.as_retriever())

query = "How good is Vicuna?"
response = qa.run(query)
clean_response = response.replace('\n', ' ').strip()
print(clean_response)

query = "How does Llama 2 outperforms other models"
qa.run(query)

while True:
  user_input = input(f"Input Prompt: ")
  if user_input == 'exit':
    print('Exiting')
    break
  if user_input == '':
    continue
  result = qa({'query': user_input})
  print(f"Answer: {result['result']}")

